# March 29 2020
# Concordia University
# COMP 472 Section NN
# Project 2
# By:
# Jason Brennan - 27793928
# Maryam Eskandari - 40065716
# Martin Grezak - 25693810

import sys
import numpy as np
from operator import itemgetter, attrgetter
from enum import Enum
from collections import defaultdict
import time

# An enum class for flagging the language in use
class Language(Enum):
    EU = 0 #BASQUE
    CA = 1 #CATALAN
    GL = 2 #GALICIAN
    ES = 3 #SPANISH
    EN = 4 #ENGLISH
    PT = 5 #PORTUGUESE

class LangModel:
    indexByVocabulary_1_dict ={}
    indexByVocabulary_2_dict ={}
    indexByVocabulary_3_dict ={}

    # default constructor
    def __init__(self):
        self.vocabulary = self.getVocabulary()
        self.ngram = self.getNgram()
        self.smoothing = self.getSmoothing()
        self.trainingFile = self.getTrainingFile()
        self.testingFile = self.getTestFile()

        # each language models below will receive a Matrix generated by the vocabulary and n-gram parameters
        # TODO: 
        # convert to dictionary
        self.table = defaultdict()
        if self.ngram == 2 or self.ngram == 3:
            self.table = defaultdict(dict)
        #self.EU = self.generateMatrix(self.ngram, self.vocabulary)
        #self.CA = self.generateMatrix(self.ngram, self.vocabulary)
        #self.GL = self.generateMatrix(self.ngram, self.vocabulary)
        #self.ES = self.generateMatrix(self.ngram, self.vocabulary)
        #self.EN = self.generateMatrix(self.ngram, self.vocabulary)
        #self.PT = self.generateMatrix(self.ngram, self.vocabulary)


    #parameterized constructor
    def __init__(self,vocabulary=-1,ngram=-1,smoothing=0,trainingFile="",testingFile=""):
        self.generateIndexByVocabulary()
        
        self.vocabularyType = vocabulary
        self.vocabulary = self.getVocabulary(vocabulary)
        self.ngram = self.getNgram(ngram)
        self.smoothing = self.getSmoothing(smoothing)
        self.trainingFile = self.getTrainingFile("training-tweets.txt")
        #self.trainingFile = self.getTrainingFile(trainingFile)

        self.testingFile = self.getTestFile("test-tweets-given.txt")
        #self.testingFile = self.getTestFile(testingFile)

         # each language models below will receive a Matrix generated by the vocabulary and n-gram parameters
        self.EU = self.generateMatrix_m(self.ngram, self.vocabulary , self.smoothing)
        self.CA = self.generateMatrix_m(self.ngram, self.vocabulary , self.smoothing)
        self.GL = self.generateMatrix_m(self.ngram, self.vocabulary , self.smoothing)
        self.ES = self.generateMatrix_m(self.ngram, self.vocabulary , self.smoothing)
        self.EN = self.generateMatrix_m(self.ngram, self.vocabulary , self.smoothing)
        self.PT = self.generateMatrix_m(self.ngram, self.vocabulary , self.smoothing)

        # test--------------------------
        self.increaseSeenEventGivenToken_MatrixModel("abc" , 0)
        self.increaseSeenEventGivenToken_MatrixModel("abc" , 0)
        self.increaseSeenEventGivenToken_MatrixModel("abc" , 0)
        self.increaseSeenEventGivenToken_MatrixModel("abc" , 0)
        self.increaseSeenEventGivenToken_MatrixModel("abc" , 0)
        print (self.getProbabilityGivenToken_MatrixModel("abc" , 0))
        print (self.getProbabilityGivenToken_MatrixModel("acc" , 0))
        #---------------------------------


        self.table = defaultdict()
        if self.ngram == 2 or self.ngram == 3:
            self.table = defaultdict(dict)

    def getVocabulary(self,vocabulary=-1):

        choice = vocabulary

        if(choice==-1):
            print("Select a number for which vocabulary you would like to use:")
            print("0 : Fold the corpus to lowercase and use only the 26 letters of the alphabet [a-z]")
            print("1 : Distinguish up and low cases and use only the 26 letters of the alphabet [a-z,A-Z]")
            print("2 : Distinguish up and low cases and use all characters accepted by the built-in isalpha() method")
            choice = int(input ("Enter your choice: "))

        switcher = {
            0: self.generateVocabulary(0),
            1: self.generateVocabulary(1),
            2: self.generateVocabulary(2)
            }
        
        return switcher.get(choice,"Invalid selection")

    def getNgram(self,ngram=-1):

        choice = ngram

        if(choice==-1):
            print("Select a number for which size n-gram you would like to use:")
            print("1 : character unigrams")
            print("2 : character bigrams")
            print("3 : character trigrams")
            choice = int(input ("Enter your choice: "))

        switcher = {
            1: 1,
            2: 2,
            3: 3
            }
        
        return switcher.get(choice,"Invalid selection")

    def getSmoothing(self,smoothing=0):

        choice = smoothing

        if(choice==0):

            interrupt = False
            count = 0
                
            while(not interrupt):
                    
                count = count + 1
                    
                choice = float(input ("Enter a smoothing value between 0 and 1 : "))
                    
                if(choice>=0 or choice<=1):
                    interrupt = True
                    
                if(count>3):
                    print("You failed to provide a smoothing value between 0 and 1, program will continue with default value: 0 ")
                    choice==0
                    interrupt = True
        
        return choice

    def getTrainingFile(self,trainingFile=""):

        dataSet = list()
        fileName = trainingFile
        count = 0

        try:
            # read the data into a list
            with open(str(fileName), encoding="utf8") as file:
                dataSet = file.readlines()

        except FileNotFoundError :
            print("File does not exist")
            fileName=""

        if(fileName==""):

            interrupt = False
            count = 0
            while(not interrupt):
                count = count + 1
                fileName = input ("Enter a valid TRAINING file name with the extension : ")
                
                try:
                    # read the data into a list
                    with open(str(fileName), encoding="utf8") as file:
                        dataSet = file.readlines()

                except FileNotFoundError :
                    print("File does not exist")

                
                if(len(dataSet)>0):
                    interrupt = True
                    
                if(count>3):
                    print("You failed to provide a valid TRAINING file, program will use default training dataset")
                    
                    fileName = "training-tweets.txt"

                    # read the data into a list
                    with open(str(fileName), encoding="utf8") as file:
                        dataSet = file.readlines()

                    interrupt = True

        return dataSet


    def getTestFile(self,testFile=""):

        dataSet = list()
        fileName = testFile
        count = 0

        try:
            # read the data into a list
            with open(str(fileName), encoding="utf8") as file:
                dataSet = file.readlines()

        except FileNotFoundError :
            print("File does not exist")
            fileName=""

        if(fileName==""):

            interrupt = False
            count = 0

            while(not interrupt):
                count = count + 1
                fileName = input ("Enter a valid TEST file name with the extension : ")
                
                try:
                    # read the data into a list
                    with open(str(fileName), encoding="utf8") as file:
                        dataSet = file.readlines()

                except FileNotFoundError :
                    print("File does not exist")

                
                if(len(dataSet)>0):
                    interrupt = True
                    
                if(count>3):
                    print("You failed to provide a valid TEST file, program will use default training dataset")
                    
                    fileName = "test-tweets-given.txt"

                        # read the data into a list
                    with open(str(fileName), encoding="utf8") as file:
                        dataSet = file.readlines()

                    interrupt = True

        return dataSet

    def generateVocabulary(self, selection):

        select = selection

        if(select==0):

            dataSet = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x','y','z']
        
        elif (select==1):

            dataSet = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x','y','z','A','B','C','D','E','F','G','H','I','J','K','L','M','N','O','P','Q','R','S','T','U','V','W','X','Y','Z']

        elif(select==2):
            
            fileName = "utf8.txt"
            dataSet = list()

            # read the data into a list
            with open(str(fileName), encoding="utf8") as file:

              while True:
                char = file.read(1)
                if not char:
                  break
                if(char.isalpha() and char!=" "):
                    dataSet.append(char)
            
            #remove duplicates from the dataSet
            if len(dataSet) == len(set(dataSet)):
                    print ("no duplicates")
            else:
                    print ("has duplicates")
                    dataSet = list(set(dataSet))
            

  
        return dataSet

    def generateMatrix(self):
        size = len(vocabulary)
 
        if(ngram==1):
            newSize = size*1
        elif(ngram==2):
            newSize = size**2
        elif(ngram==3):
            newSize = size**3
        
        return np.zeros(newSize,newSize)

    def generateMatrix_m(self,ngram,vocabulary , smoothingVal):

        size = len(vocabulary)
        ngramMatrix = np.zeros

        if(ngram==1):
            ngramMatrix = np.full(size, smoothingVal)
           
        elif(ngram==2):
            ngramMatrix = np.full((size,size), smoothingVal)
        
        elif(ngram==3):
            ngramMatrix = np.full((size,size,size), smoothingVal , dtype=np.single)
            

        return ngramMatrix
         
    def increaseSeenEventGivenToken_MatrixModel(self , token , language):
         
        switcherVocabularyType = {
            0: LangModel.indexByVocabulary_1_dict,
            1: LangModel.indexByVocabulary_2_dict,
            2: LangModel.indexByVocabulary_3_dict
            }
         
        switcherLanguage = {
             0: self.EU,
             1: self.CA,
             2: self.GL,
             3: self.ES,
             4: self.EN,
             5: self.PT
            }
       
        
        if self.ngram ==1 :
            switcherLanguage.get(language)[switcherVocabularyType.get(self.vocabularyType)[token]]+=1
        
        elif self.ngram==2:
            switcherLanguage.get(language)[switcherVocabularyType.get(self.vocabularyType)[token[0]]][switcherVocabularyType.get(self.vocabularyType)[token[1]]]+=1
        else:
          switcherLanguage.get(language)[switcherVocabularyType.get(self.vocabularyType)[token[0]]][switcherVocabularyType.get(self.vocabularyType)[token[1]]][switcherVocabularyType.get(self.vocabularyType)[token[2]]]+=1  
        
    def getProbabilityGivenToken_MatrixModel(self , token , language):
         
        switcherVocabularyType = {
            0: LangModel.indexByVocabulary_1_dict,
            1: LangModel.indexByVocabulary_2_dict,
            2: LangModel.indexByVocabulary_3_dict
            }
         
        switcherLanguage = {
             0: self.EU,
             1: self.CA,
             2: self.GL,
             3: self.ES,
             4: self.EN,
             5: self.PT
            }
       
        
        if self.ngram ==1 :
            return (switcherLanguage.get(language)[switcherVocabularyType.get(self.vocabularyType)[token]])
        
        elif self.ngram==2:
            return(switcherLanguage.get(language)[switcherVocabularyType.get(self.vocabularyType)[token[0]]][switcherVocabularyType.get(self.vocabularyType)[token[1]]])
        else:
          return(switcherLanguage.get(language)[switcherVocabularyType.get(self.vocabularyType)[token[0]]][switcherVocabularyType.get(self.vocabularyType)[token[1]]][switcherVocabularyType.get(self.vocabularyType)[token[2]]]) 
  
    def generateIndexByVocabulary(self):
        for i in range(3):
            vocabulary = self.generateVocabulary(i)
            size = len(vocabulary)
            indexDict = {}
            index = 0
            for j in range (size ):
                 indexDict[vocabulary[j]] = index
                 index+=1
            if i == 0 :
                LangModel.indexByVocabulary_1_dict =indexDict
            
            elif i==1:
                LangModel.indexByVocabulary_2_dict=indexDict
            else:
                
                LangModel.indexByVocabulary_3_dict=indexDict

    def generateDict_m(self,ngram,vocabulary , smoothingVal):

        size = len(vocabulary)
        ngramDict = {}
    
        return ngramDict
       
    def generateProbabilityTable(self):
        self.splitTrainingFile()

        # read trainingFile[i][3] character by character
        for line in self.trainingFile:
            self.incrementValues(self.stringToLanguageEnum(line[2]), line[3].lower())

    def initializeTables(self):
        if self.ngram == 1:
            for character in self.vocabulary:
                self.table[(Language.EU, character)] = self.smoothing
                self.table[(Language.CA, character)] = self.smoothing
                self.table[(Language.GL, character)] = self.smoothing
                self.table[(Language.ES, character)] = self.smoothing
                self.table[(Language.EN, character)] = self.smoothing
                self.table[(Language.PT, character)] = self.smoothing
        elif self.ngram == 2:
            for c1 in self.vocabulary:
                for c2 in self.vocabulary:
                    self.table[(Language.EU, c1)][c2] = self.smoothing
                    self.table[(Language.CA, c1)][c2] = self.smoothing
                    self.table[(Language.GL, c1)][c2] = self.smoothing
                    self.table[(Language.ES, c1)][c2] = self.smoothing
                    self.table[(Language.EN, c1)][c2] = self.smoothing
                    self.table[(Language.PT, c1)][c2] = self.smoothing
        elif self.ngram == 3:
            for c1 in self.vocabulary:
                for c2 in self.vocabulary:
                    for c3 in self.vocabulary:
                        self.table[(Language.EU, c1, c2)][c3] = self.smoothing
                        self.table[(Language.CA, c1, c2)][c3] = self.smoothing
                        self.table[(Language.GL, c1, c2)][c3] = self.smoothing
                        self.table[(Language.ES, c1, c2)][c3] = self.smoothing
                        self.table[(Language.EN, c1, c2)][c3] = self.smoothing
                        self.table[(Language.PT, c1, c2)][c3] = self.smoothing


    def splitTrainingFile(self):
        for i in range(len(self.trainingFile)):
            # split the lines in the training file by the first 3 tabs
            self.trainingFile[i] = self.trainingFile[i].split("\t", 3)
            # remove trailing newline character at the end of the tweet
            self.trainingFile[i][3] = self.trainingFile[i][3][0:len(self.trainingFile[i][3])-1]

    def stringToLanguageEnum(self, str):
        if str == "eu":
            return Language.EU
        elif str == "ca":
            return Language.CA
        elif str == "gl":
            return Language.GL
        elif str == "es":
            return Language.ES
        elif str == "en":
            return Language.EN
        elif str == "pt":
            return Language.PT

    def existsInVocab(self, str):
        for character in str:
            if character not in self.vocabulary:
                return False
        return True

    def incrementValues(self, language, str):
        if self.ngram == 1:
            for i in range(len(str) - self.ngram - 1):
                substr = str[i:(i + self.ngram)]
                if self.existsInVocab(substr):
                    self.table[(language, substr)] += 1

        elif self.ngram == 2:
            for i in range(len(str) - self.ngram - 1):
                substr = str[i:(i + self.ngram)]
                if self.existsInVocab(substr):
                    self.table[(language, substr[0])][substr[1]] += 1

        elif self.ngram == 3:
            substringLength = 3
            for i in range(len(str) - substringLength - 1):
                substr = str[i:(i + substringLength)]
                if self.existsInVocab(substr):
                    self.table[(language, substr[0], substr[1])][substr[2]] += 1
        

#MAIN

test = LangModel(0, 1, 0.5, "training-tweets.txt", "test-tweets-given.txt")


