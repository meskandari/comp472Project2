# March 29 2020
# Concordia University
# COMP 472 Section NN
# Project 2
# By:
# Jason Brennan - 27793928
# Maryam Eskandari - 40065716
# Martin Grezak - 25693810

import sys
import numpy as np
from operator import itemgetter, attrgetter
from enum import Enum
import time

# An enum class for flagging the language in use
class Language(Enum):
    EU = 0, 'Basque'
    CA = 1, 'Catalan'
    GL = 2, 'Galician'
    ES = 3, 'Spanish'
    EN = 4, 'English'
    PT = 5, 'Portugese'

    def __new__(cls, value, name):
        member = object.__new__(cls)
        member._value_ = value
        member.fullname = name
        return member

    def __int__(self):
        return self.value

class LangModel:
    indexByVocabulary_1_dict ={}
    indexByVocabulary_2_dict ={}
    indexByVocabulary_3_dict ={}

    # default constructor
    def __init__(self):
        self.vocabulary = self.getVocabulary()
        self.ngram = self.getNgram()
        self.smoothing = self.getSmoothing()
        self.trainingFile = self.getTrainingFile()
        self.testingFile = self.getTestFile()

        # each language models below will receive a Matrix generated by the vocabulary and n-gram parameters
        #self.EU = self.generateMatrix(self.ngram, self.vocabulary)
        #self.CA = self.generateMatrix(self.ngram, self.vocabulary)
        #self.GL = self.generateMatrix(self.ngram, self.vocabulary)
        #self.ES = self.generateMatrix(self.ngram, self.vocabulary)
        #self.EN = self.generateMatrix(self.ngram, self.vocabulary)
        #self.PT = self.generateMatrix(self.ngram, self.vocabulary)


    #parameterized constructor
    def __init__(self,vocabulary=-1,ngram=-1,smoothing=0,trainingFile="",testingFile=""):
        self.generateIndexByVocabulary()
        self.vocabularyType = vocabulary
        self.vocabulary = self.getVocabulary(vocabulary)
        self.ngram = self.getNgram(ngram)
        self.smoothing = self.getSmoothing(smoothing)
        self.trainingFile = self.getTrainingFile("training-tweets.txt")
        #self.trainingFile = self.getTrainingFile(trainingFile)

        self.testingFile = self.getTestFile("test-tweets-given.txt")
        #self.testingFile = self.getTestFile(testingFile)

         # each language models below will receive a Matrix generated by the vocabulary and n-gram parameters
        self.EU = self.generateMatrix_m(self.ngram, self.vocabulary , self.smoothing)
        self.CA = self.generateMatrix_m(self.ngram, self.vocabulary , self.smoothing)
        self.GL = self.generateMatrix_m(self.ngram, self.vocabulary , self.smoothing)
        self.ES = self.generateMatrix_m(self.ngram, self.vocabulary , self.smoothing)
        self.EN = self.generateMatrix_m(self.ngram, self.vocabulary , self.smoothing)
        self.PT = self.generateMatrix_m(self.ngram, self.vocabulary , self.smoothing)

        # test--------------------------
        #self.increaseSeenEventGivenToken_MatrixModel("abc" , 0)
        #self.increaseSeenEventGivenToken_MatrixModel("abc" , 0)
        #self.increaseSeenEventGivenToken_MatrixModel("abc" , 0)
        #self.increaseSeenEventGivenToken_MatrixModel("abc" , 0)
        #self.increaseSeenEventGivenToken_MatrixModel("abc" , 0)
        #print (self.getProbabilityGivenToken_MatrixModel("abc" , 0))
        #print (self.getProbabilityGivenToken_MatrixModel("acc" , 0))
        #---------------------------------

    def getVocabulary(self,vocabulary=-1):

        choice = vocabulary

        if(choice==-1):
            print("Select a number for which vocabulary you would like to use:")
            print("0 : Fold the corpus to lowercase and use only the 26 letters of the alphabet [a-z]")
            print("1 : Distinguish up and low cases and use only the 26 letters of the alphabet [a-z,A-Z]")
            print("2 : Distinguish up and low cases and use all characters accepted by the built-in isalpha() method")
            choice = int(input ("Enter your choice: "))

        switcher = {
            0: self.generateVocabulary(0),
            1: self.generateVocabulary(1),
            2: self.generateVocabulary(2)
            }
        
        return switcher.get(choice,"Invalid selection")

    def getNgram(self,ngram=-1):

        choice = ngram

        if(choice==-1):
            print("Select a number for which size n-gram you would like to use:")
            print("1 : character unigrams")
            print("2 : character bigrams")
            print("3 : character trigrams")
            choice = int(input ("Enter your choice: "))

        switcher = {
            1: 1,
            2: 2,
            3: 3
            }
        
        return switcher.get(choice,"Invalid selection")

    def getSmoothing(self,smoothing=0):

        choice = smoothing

        if(choice==0):

            interrupt = False
            count = 0
                
            while(not interrupt):
                    
                count = count + 1
                    
                choice = float(input ("Enter a smoothing value between 0 and 1 : "))
                    
                if(choice>=0 or choice<=1):
                    interrupt = True
                    
                if(count>3):
                    print("You failed to provide a smoothing value between 0 and 1, program will continue with default value: 0 ")
                    choice==0
                    interrupt = True
        
        return choice

    def getTrainingFile(self,trainingFile=""):

        dataSet = list()
        fileName = trainingFile
        count = 0

        try:
            # read the data into a list
            with open(str(fileName), encoding="utf8") as file:
                dataSet = file.readlines()

        except FileNotFoundError :
            print("File does not exist")
            fileName=""

        if(fileName==""):

            interrupt = False
            count = 0
            while(not interrupt):
                count = count + 1
                fileName = input ("Enter a valid TRAINING file name with the extension : ")
                
                try:
                    # read the data into a list
                    with open(str(fileName), encoding="utf8") as file:
                        dataSet = file.readlines()

                except FileNotFoundError :
                    print("File does not exist")

                
                if(len(dataSet)>0):
                    interrupt = True
                    
                if(count>3):
                    print("You failed to provide a valid TRAINING file, program will use default training dataset")
                    
                    fileName = "training-tweets.txt"

                    # read the data into a list
                    with open(str(fileName), encoding="utf8") as file:
                        dataSet = file.readlines()

                    interrupt = True

        return dataSet


    def getTestFile(self,testFile=""):

        dataSet = list()
        fileName = testFile
        count = 0

        try:
            # read the data into a list
            with open(str(fileName), encoding="utf8") as file:
                dataSet = file.readlines()

        except FileNotFoundError :
            print("File does not exist")
            fileName=""

        if(fileName==""):

            interrupt = False
            count = 0

            while(not interrupt):
                count = count + 1
                fileName = input ("Enter a valid TEST file name with the extension : ")
                
                try:
                    # read the data into a list
                    with open(str(fileName), encoding="utf8") as file:
                        dataSet = file.readlines()

                except FileNotFoundError :
                    print("File does not exist")

                
                if(len(dataSet)>0):
                    interrupt = True
                    
                if(count>3):
                    print("You failed to provide a valid TEST file, program will use default training dataset")
                    
                    fileName = "test-tweets-given.txt"

                        # read the data into a list
                    with open(str(fileName), encoding="utf8") as file:
                        dataSet = file.readlines()

                    interrupt = True

        return dataSet

    def generateVocabulary(self, selection):

        select = selection

        if(select==0):

            dataSet = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x','y','z']
        
        elif (select==1):

            dataSet = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x','y','z','A','B','C','D','E','F','G','H','I','J','K','L','M','N','O','P','Q','R','S','T','U','V','W','X','Y','Z']

        elif(select==2):
            
            fileName = "utf8.txt"
            dataSet = list()

            # read the data into a list
            with open(str(fileName), encoding="utf8") as file:

              while True:
                char = file.read(1)
                if not char:
                  break
                if(char.isalpha() and char!=" "):
                    dataSet.append(char)
            
            #remove duplicates from the dataSet
            if len(dataSet) == len(set(dataSet)):
                    print ("no duplicates")
            else:
                    print ("has duplicates")
                    dataSet = list(set(dataSet))
  
        return dataSet

    def generateMatrix(self):
        size = len(vocabulary)
 
        if(ngram==1):
            newSize = size*1
        elif(ngram==2):
            newSize = size**2
        elif(ngram==3):
            newSize = size**3
        
        return np.zeros(newSize,newSize)

    def generateMatrix_m(self,ngram,vocabulary , smoothingVal):

        size = len(vocabulary)
        ngramMatrix = np.zeros

        if(ngram==1):
            ngramMatrix = np.full(size + 1, smoothingVal)
            ngramMatrix[-1] = size * smoothingVal
           
        elif(ngram==2):
            ngramMatrix = np.full((size,size+1), smoothingVal, dtype=np.half)
            for row in ngramMatrix:
                row[-1] = size * smoothingVal
        
        elif(ngram==3):
            ngramMatrix = np.full((size,size,size+1), smoothingVal , dtype=np.half)
            for row in ngramMatrix:
                for depth in row:
                    depth[-1] = size * smoothingVal

        return ngramMatrix
         
    def increaseSeenEventGivenToken_MatrixModel(self , token , language):
         
        switcherVocabularyType = {
            0: LangModel.indexByVocabulary_1_dict,
            1: LangModel.indexByVocabulary_2_dict,
            2: LangModel.indexByVocabulary_3_dict
            }
         
        switcherLanguage = {
             0: self.EU,
             1: self.CA,
             2: self.GL,
             3: self.ES,
             4: self.EN,
             5: self.PT
            }
       
        
        if self.ngram ==1 :
            table = switcherLanguage.get(language)
            table[switcherVocabularyType.get(self.vocabularyType)[token]]+=1
            table[-1] += 1
        
        elif self.ngram==2:
            table = switcherLanguage.get(language)
            row = table[switcherVocabularyType.get(self.vocabularyType)[token[0]]]
            row[switcherVocabularyType.get(self.vocabularyType)[token[1]]]+=1
            row[-1] += 1

        else:
            table = switcherLanguage.get(language)
            row = table[switcherVocabularyType.get(self.vocabularyType)[token[0]]]
            depth = row[switcherVocabularyType.get(self.vocabularyType)[token[1]]]
            depth[switcherVocabularyType.get(self.vocabularyType)[token[2]]]+=1
            depth[-1] += 1
        
    def getProbabilityGivenToken_MatrixModel(self , token , language):
         
        switcherVocabularyType = {
            0: LangModel.indexByVocabulary_1_dict,
            1: LangModel.indexByVocabulary_2_dict,
            2: LangModel.indexByVocabulary_3_dict
            }
         
        switcherLanguage = {
             0: self.EU,
             1: self.CA,
             2: self.GL,
             3: self.ES,
             4: self.EN,
             5: self.PT
            }
       
        
        if self.ngram ==1 :
            return (switcherLanguage.get(language)[switcherVocabularyType.get(self.vocabularyType)[token]])
        
        elif self.ngram==2:
            return(switcherLanguage.get(language)[switcherVocabularyType.get(self.vocabularyType)[token[0]]][switcherVocabularyType.get(self.vocabularyType)[token[1]]])
        else:
          return(switcherLanguage.get(language)[switcherVocabularyType.get(self.vocabularyType)[token[0]]][switcherVocabularyType.get(self.vocabularyType)[token[1]]][switcherVocabularyType.get(self.vocabularyType)[token[2]]]) 
  
    def generateIndexByVocabulary(self):
        for i in range(3):
            vocabulary = self.generateVocabulary(i)
            size = len(vocabulary)
            indexDict = {}
            index = 0
            for j in range (size ):
                 indexDict[vocabulary[j]] = index
                 index+=1
            if i == 0 :
                LangModel.indexByVocabulary_1_dict =indexDict
            
            elif i==1:
                LangModel.indexByVocabulary_2_dict=indexDict
            else:
                
                LangModel.indexByVocabulary_3_dict=indexDict

    def generateDict_m(self,ngram,vocabulary , smoothingVal):

        size = len(vocabulary)
        ngramDict = {}
    
        return ngramDict

    def generateProbabilityTable(self):
        self.splitTrainingFile()

        # read trainingFile[i][3] character by character, convert to lower case if using vocabulary 0
        if self.vocabularyType == 0:
            for line in self.trainingFile:
                self.parseNgrams(self.stringToLanguageEnum(line[2]), line[3].lower())
        else:
            for line in self.trainingFile:
                self.parseNgrams(self.stringToLanguageEnum(line[2]), line[3])

    def splitTrainingFile(self):
        for i in range(len(self.trainingFile)):
            # split the lines in the training file by the first 3 tabs
            self.trainingFile[i] = self.trainingFile[i].split("\t", 3)

            # remove trailing newline character at the end of the tweet
            self.trainingFile[i][3] = self.trainingFile[i][3][0:len(self.trainingFile[i][3])-1]

    def stringToLanguageEnum(self, str):
        if str == "eu":
            return Language.EU
        elif str == "ca":
            return Language.CA
        elif str == "gl":
            return Language.GL
        elif str == "es":
            return Language.ES
        elif str == "en":
            return Language.EN
        elif str == "pt":
            return Language.PT

    def existsInVocab(self, str):
        for character in str:
            if character not in self.vocabulary:
                return False
        return True

    def parseNgrams(self, language, str):
        if self.ngram == 1:
            for i in range(len(str) - self.ngram):
                substr = str[i:(i + self.ngram)]
                if self.existsInVocab(substr):
                    self.increaseSeenEventGivenToken_MatrixModel(substr, int(language))

        elif self.ngram == 2:
            for i in range(len(str) - self.ngram):
                substr = str[i:(i + self.ngram)]
                if self.existsInVocab(substr):
                    self.increaseSeenEventGivenToken_MatrixModel(substr, int(language))

        elif self.ngram == 3:
            for i in range(len(str) - self.ngram):
                substr = str[i:(i + self.ngram)]
                if self.existsInVocab(substr):
                    self.increaseSeenEventGivenToken_MatrixModel(substr, int(language))

    def printResults(self,listResults, byomFlag=0):

        #----------------Trace File Section-----------------------------
        
        #Metrics for Accuracy
        countWrong, countCorrect = 0

        #Metrics for True Positive
        metricsDict = {'eu':{'truePositive':0, 'falsePositive':0,'falseNegative':0},
                       'ca':{'truePositive':0, 'falsePositive':0,'falseNegative':0},
                       'gl':{'truePositive':0, 'falsePositive':0,'falseNegative':0},
                       'es':{'truePositive':0, 'falsePositive':0,'falseNegative':0},
                       'en':{'truePositive':0, 'falsePositive':0,'falseNegative':0},
                       'pt':{'truePositive':0, 'falsePositive':0,'falseNegative':0}}
               
        #Compose file name
        traceFileName = "trace_" + str(len(self.vocabulary)) +"_"+ str(self.ngram) +"_"+ str(self.smoothing) +".txt"
        
        #Hard code file name for Build Your Own Model
        if byomFlag == 1:
            traceFileName = "trace_myModel.txt"
            
        file = open(traceFileName, 'w')

        for i in range(len(self.testingFile)):
            result = self.processTweet(self.testingFile[i])
       
            tweetID = str(result[0])
            mostLikelyClass = str(result[1])
            mostLikelyScore = str(result[2])
            correctClass = str(result[3])
            outcome = str(result[4]) 

            traceOutputString = tweetID + "  " + mostLikelyClass + "  " + mostLikelyScore + "  " + correctClass + "  " + outcome + "\n"
            file.write(traceOutputString)

            if outcome=="correct":
                
                countCorrect = countCorrect + 1
            
                metricsDict[correctClass]['truePositive'] = metricsDict[correctClass]['truePositive'] + 1


            else:
                
                countWrong = countWrong + 1

                metricsDict[mostLikelyClass]['falsePositive'] = metricsDict[mostLikelyClass]['falsePositive'] + 1


                metricsDict[correctClass]['falseNegative'] = metricsDict[correctClass]['falseNegative'] + 1




        #Finally
        file.close()

        #----------------Overall Evaluation File Section ---------------

        #Metrics for Precision
        eu_P, ca_P, gl_P, es_P, en_P, pt_P = 0.00

        #Metrics for Recall
        eu_R, ca_R, gl_R, es_R, en_R, pt_R = 0.00


        #Metrics for F1-measure
        eu_F, ca_F, gl_F, es_F, en_F, pt_F = 0.00

        #macro-F1 & weighted-average-F1
        macroF1, weightedAvgF1 = 0.00

        #Compose file name
        evalFileName = "eval_" + str(len(self.vocabulary)) +"_"+ str(self.ngram) +"_"+ str(self.smoothing) +".txt"
        
        #Hard code file name for Build Your Own Model
        if byomFlag == 1:
            evalFileName = "eval_myModel.txt"

        file = open(evalFileName, 'w')

        #in loop
        evalOutputString = str(value.fn) + " " + str(value.gn) + " " + str(value.hn) + " " + str(key) + "\n"
        file.write(evalOutputString)

        #Finally
        file.close()
        

#MAIN

test = LangModel(0, 3)
test.generateProbabilityTable()

print(test.EU)
